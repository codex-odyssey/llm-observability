import os, uuid
from dotenv import load_dotenv, find_dotenv
import vector_store as vs

import streamlit as st
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai.chat_models import ChatOpenAI
from langchain_cohere.chat_models import ChatCohere
from langfuse import Langfuse
from langfuse.callback import CallbackHandler

_ = load_dotenv(find_dotenv())
openai_api_key = os.getenv("OPENAI_API_KEY")
cohere_api_key = os.getenv("COHERE_API_KEY")
endpoint = os.getenv("ENDPOINT")
public_key = os.getenv("PUBLIC_KEY")
secret_key = os.getenv("SECRET_KEY")
fallback_prompt = """
ä»¥ä¸‹ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

## è³ªå•
{{question}}
"""

# ç°¡æ˜“çš„ãªã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
session_id = str(uuid.uuid4())
if "session_id" not in st.session_state:
    st.session_state["session_id"] = str(uuid.uuid4())
session_id = st.session_state["session_id"]

# Langfuseé–¢é€£
langfuse = Langfuse(public_key=public_key, secret_key=secret_key, host=endpoint)
langchain_callback = CallbackHandler(
    host=endpoint, public_key=public_key, secret_key=secret_key, session_id=session_id
)

# Vector Storeã®åˆæœŸåŒ–
vector_store = vs.initialize()

st.title("ğŸ– Ask the BigBaBy ğŸ–")
st.caption(
    """
æŠ€è¡“æ›¸å…¸#17 - ä¿ºãŸã¡ã®LLM Observabilityæœ¬ã®ãƒãƒ³ã‚ºã‚ªãƒ³ã§ä½¿ç”¨ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã™ã€‚  
ä»Šæ—¥ã®æ™©å¾¡é£¯ã®çŒ®ç«‹ã«å›°ã£ãŸæ™‚ã«ã”æ´»ç”¨ãã ã•ã„ã€‚  
OpenAI GPT-4o, Cohere Command R+ã®ã©ã¡ã‚‰ã‹ã‚’é¸ã‚“ã§åˆ©ç”¨å¯èƒ½ã§ã™ã€‚
"""
)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼é–¢é€£
with st.sidebar.container():
    with st.sidebar:
        model_name = st.sidebar.selectbox(
            label="Model Name", options=["command-r-plus", "gpt-4o-mini"]
        )
        max_tokens = st.sidebar.slider(
            label="Max Tokens",
            min_value=128,
            max_value=2048,
            value=1024,
            step=128,
            help="LLMãŒå‡ºåŠ›ã™ã‚‹æœ€å¤§ã®ãƒˆãƒ¼ã‚¯ãƒ³é•·",
        )
        temperature = st.sidebar.slider(
            label="Temperature",
            min_value=0.0,
            max_value=1.0,
            value=0.7,
            step=0.1,
            help="ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§",
        )


def generate_response(query: str):
    """Generate LLM response via streaming output."""
    if model_name == "gpt-4o-mini":
        chat_model = ChatOpenAI(
            api_key=openai_api_key,
            model=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
        )
    elif model_name == "command-r-plus":
        chat_model = ChatCohere(
            cohere_api_key=cohere_api_key,
            model_name=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
        )
    prompt = langfuse.get_prompt(
        name="bbql-app-prompt", fallback=fallback_prompt
    ).get_langchain_prompt()
    chain = (
        {"question": RunnablePassthrough(), "context": vector_store.as_retriever()}
        | PromptTemplate.from_template(prompt)
        | chat_model
        | StrOutputParser()
    )
    response = chain.stream(input=query, config={"callbacks": [langchain_callback]})
    for chunk in response:
        yield chunk


if "messages" not in st.session_state:
    st.session_state.messages = []
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ä»Šæ—¥ã¯ä½•ãŒé£Ÿã¹ãŸã„æ°—åˆ†ã§ã™ã‹ï¼Ÿ"):
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("assistant"):
        messages = [
            {"role": message["role"], "content": message["content"]}
            for message in st.session_state.messages
        ]
        stream = generate_response(query=prompt)
        response = st.write_stream(stream=stream)
    st.session_state.messages.append({"role": "assistant", "content": response})
