import os, uuid, logging
from dotenv import load_dotenv, find_dotenv
import vector_store as vs

import streamlit as st
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain_openai.chat_models import ChatOpenAI
from langchain_cohere.chat_models import ChatCohere
from langchain_cohere.rerank import CohereRerank
from langfuse import Langfuse
from langfuse.callback import CallbackHandler

logger = logging.getLogger(name=__name__)

_ = load_dotenv(find_dotenv())
openai_api_key = os.getenv("OPENAI_API_KEY")
cohere_api_key = os.getenv("COHERE_API_KEY")
endpoint = os.getenv("LANGFUSE_ENDPOINT")
public_key = os.getenv("LANGFUSE_PUBLIC_KEY")
secret_key = os.getenv("LANGFUSE_SECRET_KEY")
fallback_prompt = """
ä»¥ä¸‹ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

## è³ªå•
{{question}}
"""

# ç°¡æ˜“çš„ãªã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
if "session_id" not in st.session_state:
    st.session_state["session_id"] = str(uuid.uuid4())
session_id = st.session_state["session_id"]

# Langfuseé–¢é€£
if "langfuse" not in st.session_state:
    st.session_state["langfuse"] = Langfuse(
        public_key=public_key, secret_key=secret_key, host=endpoint
    )
langfuse = st.session_state["langfuse"]
if "langchain_callback" not in st.session_state:
    st.session_state["langchain_callback"] = CallbackHandler(
        host=endpoint,
        public_key=public_key,
        secret_key=secret_key,
        session_id=session_id,
        trace_name="Ask the BigBaBy",
        tags=["app"],
    )
langchain_callback = st.session_state["langchain_callback"]

st.title("ğŸ– Ask the BigBaBy ğŸ–")
st.caption(
    """
æŠ€è¡“æ›¸å…¸#17 - ä¿ºãŸã¡ã®LLM Observabilityæœ¬ã®ãƒãƒ³ã‚ºã‚ªãƒ³ã§ä½¿ç”¨ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã™ã€‚  
ä»Šæ—¥ã®æ™©å¾¡é£¯ã®çŒ®ç«‹ã«å›°ã£ãŸæ™‚ã«ã”æ´»ç”¨ãã ã•ã„ã€‚  
OpenAI GPT-4o, Cohere Command R+ã®ã©ã¡ã‚‰ã‹ã‚’é¸ã‚“ã§åˆ©ç”¨å¯èƒ½ã§ã™ã€‚
"""
)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼é–¢é€£
with st.sidebar.container():
    with st.sidebar:
        st.sidebar.markdown("### LLMé–¢é€£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
        model_name = st.sidebar.selectbox(
            label="Model Name",
            options=["command-r-plus", "gpt-4o-mini"],
        )
        max_tokens = st.sidebar.slider(
            label="Max Tokens",
            min_value=128,
            max_value=2048,
            value=1024,
            step=128,
            help="LLMãŒå‡ºåŠ›ã™ã‚‹æœ€å¤§ã®ãƒˆãƒ¼ã‚¯ãƒ³é•·",
        )
        temperature = st.sidebar.slider(
            label="Temperature",
            min_value=0.0,
            max_value=1.0,
            value=0.7,
            step=0.1,
            help="ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§",
        )
        st.sidebar.markdown("### æ¤œç´¢é–¢é€£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
        top_k = st.sidebar.slider(
            label="Top K",
            min_value=1,
            max_value=10,
            value=5,
            step=1,
            help="é–¢é€£æƒ…å ±ã®å–å¾—æ•°",
        )
        use_reranker = st.sidebar.radio(
            label="Reranker",
            options=[True, False],
            horizontal=True,
            help="Rerankerã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆâ€»ä½¿ç”¨ã«ã¯ã€Cohereã®API KeyãŒå¿…è¦ã§ã™ï¼‰",
        )
        top_n = st.sidebar.slider(
            label="Top N",
            min_value=1,
            max_value=10,
            value=3,
            help="Rerankerã§å–å¾—ã—ãŸæƒ…å ±ã‚’ä½•ä»¶ã«çµã‚Šè¾¼ã‚€ã‹",
        )


def generate_response(query: str):
    """Generate LLM response via streaming output."""
    if model_name == "gpt-4o-mini":
        chat_model = ChatOpenAI(
            api_key=openai_api_key,
            model=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
        )
    elif model_name == "command-r-plus":
        chat_model = ChatCohere(
            cohere_api_key=cohere_api_key,
            model=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
        )
    else:
        logger.error("Unsetted model name")

    # è»½å¾®ãªå‡¦ç†ãªã®ã§ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œã”ã¨ã«åˆæœŸåŒ–ã™ã‚‹
    vector_store = vs.initialize(model_name=model_name, session_id=session_id)
    retriever = vector_store.as_retriever(search_kwargs={"k": top_k})

    # Rerankerã®ä½¿ç”¨ãƒ•ãƒ©ã‚°ãŒæœ‰åŠ¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ã®å ´åˆã¯ã€Cohereã®Rerankerã‚’ç”¨ã„ã¦ã€
    # å–å¾—ã—ãŸæƒ…å ±ã‚’é–¢é€£åº¦é †ã«ä¸¦ã³æ›¿ãˆãŸå¾Œã«ã€æŒ‡å®šä»¶æ•°åˆ†ã®ã¿æ¡ç”¨ã™ã‚‹
    if use_reranker == True:
        compressor = CohereRerank(
            cohere_api_key=cohere_api_key, model="rerank-multilingual-v3.0", top_n=top_n
        )
        retriever = ContextualCompressionRetriever(
            base_compressor=compressor, base_retriever=retriever
        )

    # bbql-app-promptã‚’Langfuseä¸Šã§ä½œæˆã™ã‚‹ã¨ã€å›ç­”ãŒå¤‰ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹
    prompt = langfuse.get_prompt(
        name="bbql-app-prompt", fallback=fallback_prompt
    ).get_langchain_prompt()

    chain = (
        {"question": RunnablePassthrough(), "context": retriever}
        | PromptTemplate.from_template(prompt)
        | chat_model
        | StrOutputParser()
    )
    response = chain.stream(input=query, config={"callbacks": [langchain_callback]})
    for chunk in response:
        yield chunk


if "messages" not in st.session_state:
    st.session_state.messages = []
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ä»Šæ—¥ã¯ä½•ãŒé£Ÿã¹ãŸã„æ°—åˆ†ã§ã™ã‹ï¼Ÿ"):
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("assistant"):
        messages = [
            {"role": message["role"], "content": message["content"]}
            for message in st.session_state.messages
        ]
        stream = generate_response(query=prompt)
        response = st.write_stream(stream=stream)
    st.session_state.messages.append({"role": "assistant", "content": response})
