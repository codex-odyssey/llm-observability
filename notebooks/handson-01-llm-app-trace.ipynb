{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ハンズオン01: LLM アプリケーション Trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必要なライブラリをダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ハンズオンに必要な環境変数を読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "endpoint = \"http://langfuse-server:3000\"\n",
    "public_key = os.getenv(\"PUBLIC_KEY\")\n",
    "secret_key = os.getenv(\"SECRET_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langfuseのクライアントを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "langfuse = Langfuse(\n",
    "    public_key=public_key,\n",
    "    secret_key=secret_key,\n",
    "    host=endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 出力に対する評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM as a Judgeの対象となる生成結果の一覧を取得します。今回は、現在時から24時間以内に生成された生成結果を評価対象として扱います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "generations = langfuse.get_generations(\n",
    "    from_start_time=datetime.datetime.now() - datetime.timedelta(hours=24),\n",
    ")\n",
    "\n",
    "pprint(f\"Fetched {len(generations.data)} generations.\")\n",
    "pprint(f\"{generations.data[0].__dict__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "評価用の関数を実装します。今回は、LangChainのEvaluatorを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.loading import load_evaluator\n",
    "from langchain.evaluation.schema import EvaluatorType\n",
    "\n",
    "def load_evaluator_by_criteria_key(key: str):\n",
    "    if os.getenv(\"COHERE_API_KEY\") == None:\n",
    "        from langchain_openai.chat_models import ChatOpenAI\n",
    "        openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        llm = ChatOpenAI(api_key=openai_api_key, model=\"gpt-4o-mini\")\n",
    "    else:\n",
    "        from langchain_cohere.chat_models import ChatCohere\n",
    "        cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "        llm = ChatCohere(cohere_api_key=cohere_api_key, model=\"command-r-plus\")\n",
    "\n",
    "    evaluator = load_evaluator(\n",
    "        evaluator=EvaluatorType.CRITERIA,\n",
    "        llm=llm,\n",
    "        criteria=key\n",
    "    )\n",
    "    return evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "評価基準を設定します。今回は、\n",
    "\n",
    "- conciseness: 簡潔で要点をついた回答であるか\n",
    "- coherence: 構造化され、整理された回答であるか\n",
    "- harmfulness: 有害、攻撃的、不適切な回答であるか\n",
    "\n",
    "を評価基準として設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterias = [\n",
    "    \"conciseness\",\n",
    "    \"coherence\",\n",
    "    \"harmfulness\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24時間以内の生成結果に対して、実際にLLMによる評価を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_evaluation_and_scoring():\n",
    "    for generation in generations.data:\n",
    "        for key in criterias:\n",
    "            evaluator = load_evaluator_by_criteria_key(key=key)\n",
    "            result = evaluator.evaluate_strings(\n",
    "                prediction=generation.output,\n",
    "                input=generation.input\n",
    "            )\n",
    "            pprint(result)\n",
    "            langfuse.score(\n",
    "                name=f\"llm-as-a-judge-{key}\",\n",
    "                trace_id=generation.trace_id,\n",
    "                observation_id=generation.id,\n",
    "                value=result.get(\"score\"),\n",
    "                comment=result.get(\"reasoning\")\n",
    "            )\n",
    "\n",
    "execute_evaluation_and_scoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （オプション）入力に対する評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM as a Judgeの対象となる一覧を取得します。今回は、現在時から24時間以内に入力されたプロンプトを対象として扱います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "traces = langfuse.get_traces(\n",
    "    from_timestamp=datetime.datetime.now() - datetime.timedelta(hours=24),\n",
    "    tags=[\"app\"]\n",
    ")\n",
    "\n",
    "pprint(f\"Fetched {len(traces.data)} generations.\")\n",
    "pprint(f\"{traces.data[0].__dict__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "評価用の関数を実装します。今回は、ユーザーの入力プロンプトを”否定的”、”中立的”、”肯定的”にLLMを用いて分類を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "fallback_prompt = \"\"\"\n",
    "以下の入力テキストを”否定的”、”中立的”、”肯定的”に分類してください。\n",
    "また、出力は”否定的”、”中立的”、”肯定的”のみで理由などは含まないでください。\n",
    "\n",
    "## 入力テキスト\n",
    "\n",
    "{{input}}\n",
    "\"\"\"\n",
    "\n",
    "def sentiment_analysis(input: str) -> str:\n",
    "    if os.getenv(\"COHERE_API_KEY\") == None:\n",
    "        from langchain_openai.chat_models import ChatOpenAI\n",
    "        openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        llm = ChatOpenAI(api_key=openai_api_key, model=\"gpt-4o-mini\")\n",
    "    else:\n",
    "        from langchain_cohere.chat_models import ChatCohere\n",
    "        cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "        llm = ChatCohere(cohere_api_key=cohere_api_key, model=\"command-r-plus\")\n",
    "    prompt = langfuse.get_prompt(name=\"sentiment-analysis-prompt\", fallback=fallback_prompt)\n",
    "    sentiment_analysis_chain = (\n",
    "        {\"input\": RunnablePassthrough()}\n",
    "        | PromptTemplate.from_template(prompt.get_langchain_prompt())\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    result = sentiment_analysis_chain.invoke(input)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力プロンプトに対する感情分析を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sentiment_analysis():\n",
    "    for trace in traces.data:\n",
    "        result = sentiment_analysis(input=trace.input)\n",
    "        score_map = {\n",
    "            \"否定的\": 0,\n",
    "            \"中立的\": 0.5,\n",
    "            \"肯定的\": 1\n",
    "        }\n",
    "        pprint({\"input\": trace.input, \"result\": result})\n",
    "        langfuse.score(\n",
    "            name=f\"llm-as-a-judge-sentiment-analysis\",\n",
    "            trace_id=trace.id,\n",
    "            observation_id=trace.id,\n",
    "            value=score_map.get(result, 0.5),\n",
    "            comment=result\n",
    "        )\n",
    "\n",
    "execute_sentiment_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
