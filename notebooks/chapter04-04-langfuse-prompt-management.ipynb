{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## オブザーバビリティを高める機能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必要なライブラリをダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行に必要な環境変数を読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, time\n",
    "\n",
    "# おまじない\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "endpoint = \"http://langfuse-server:3000\"\n",
    "public_key = os.getenv(\"PUBLIC_KEY\")\n",
    "secret_key = os.getenv(\"SECRET_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### プロンプト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各トレースを同一グループにするために、セッションIDをUUID v4ベースで生成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "# Initialize Langfuse client\n",
    "langfuse = Langfuse(\n",
    "    public_key=public_key,\n",
    "    secret_key=secret_key,\n",
    "    host=endpoint\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "プロンプトテンプレートの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langfuse.model.TextPromptClient at 0x7fc8727c2a10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a text prompt\n",
    "langfuse.create_prompt(\n",
    "    name=\"test-sdk\",\n",
    "    type=\"text\",\n",
    "    prompt=\"{{topic}}について教えて\",\n",
    "    labels=[\"sdk\",\"production\"],\n",
    "    config={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langfuse.model.TextPromptClient at 0x7fc8727f6bd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langfuse.create_prompt(\n",
    "    name=\"test-sdk\",\n",
    "    type=\"text\",\n",
    "    prompt=\"簡潔に{{topic}}について教えて\",\n",
    "    labels=[\"sdk\",\"production\"],\n",
    "    config={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'大爆笑一発ギャグについて教えて'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# プロンプトテンプレート`test-sdk\"`を取得\n",
    "langfuse_prompt = langfuse.get_prompt(\"test-sdk\",version=1)\n",
    "\n",
    "# 取得したプロンプトから\n",
    "compiled_prompt = langfuse_prompt.compile(topic=\"大爆笑一発ギャグ\")\n",
    "# -> \"As an expert movie critic, do you like Dune 2?\"\n",
    "compiled_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "session_id = str(uuid.uuid4())\n",
    "\n",
    "from langfuse.decorators import langfuse_context, observe\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_cohere import ChatCohere\n",
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "callback_handler = CallbackHandler(\n",
    "    public_key=public_key,\n",
    "    secret_key=secret_key,\n",
    "    host=endpoint,\n",
    "    session_id=session_id,\n",
    "    user_id=\"user2@example.com\"\n",
    ")\n",
    "#@observe#(as_type=\"generation\")\n",
    "\n",
    "@observe()\n",
    "def nested_generation():\n",
    "    langchain_prompt = ChatPromptTemplate.from_template(\n",
    "        langfuse_prompt.get_langchain_prompt(),\n",
    "        metadata={\"langfuse_prompt\": langfuse_prompt},\n",
    "    )\n",
    "    langfuse_context.update_current_observation(\n",
    "        prompt=langchain_prompt,\n",
    "    )\n",
    "    model = ChatCohere(\n",
    "        model=\"command-r-plus\",\n",
    "        temperature=1,\n",
    "    )\n",
    "    chain = langchain_prompt | model\n",
    "    chain.invoke(\n",
    "        input={\"topic\": \"大爆笑一発ギャグ\"},\n",
    "        config={\"callbacks\": [callback_handler]},\n",
    "    )\n",
    "\n",
    "    #return langchain_prompt\n",
    "\n",
    "nested_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(id='cm2ki8l2g001bu5nhzgiebjij', name='test', description='My first dataset', metadata={'date': '2022-01-01', 'type': 'benchmark', 'author': 'Alice'}, project_id='pj-1234567890', created_at=datetime.datetime(2024, 10, 22, 13, 52, 11, 945000, tzinfo=datetime.timezone.utc), updated_at=datetime.datetime(2024, 10, 25, 18, 30, 21, 82000, tzinfo=datetime.timezone.utc))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langfuse.create_dataset(\n",
    "    name=\"test\",\n",
    "    # optional description\n",
    "    description=\"My first dataset\",\n",
    "    # optional metadata\n",
    "    metadata={\n",
    "        \"author\": \"Alice\",\n",
    "        \"date\": \"2022-01-01\",\n",
    "        \"type\": \"benchmark\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetItem(id='ef21215a-a7c3-4cdf-8b5d-1d1824678a49', status=<DatasetStatus.ACTIVE: 'ACTIVE'>, input={'text': 'hello world'}, expected_output={'text': 'hello world'}, metadata={'model': 'llama3'}, source_trace_id=None, source_observation_id=None, dataset_id='cm2ki8l2g001bu5nhzgiebjij', dataset_name='test', created_at=datetime.datetime(2024, 10, 25, 18, 30, 21, 97000, tzinfo=datetime.timezone.utc), updated_at=datetime.datetime(2024, 10, 25, 18, 30, 21, 97000, tzinfo=datetime.timezone.utc))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langfuse.create_dataset_item(\n",
    "    dataset_name=\"test\",\n",
    "    # any python object or value, optional\n",
    "    input={\n",
    "        \"text\": \"hello world\"\n",
    "    },\n",
    "    # any python object or value, optional\n",
    "    expected_output={\n",
    "        \"text\": \"hello world\"\n",
    "    },\n",
    "    # metadata, optional\n",
    "    metadata={\n",
    "        \"model\": \"llama3\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetItem(id='de4bc98c-a6b8-48ec-906c-e8ecae909329', status=<DatasetStatus.ACTIVE: 'ACTIVE'>, input={'text': 'hello world'}, expected_output={'text': 'hello world'}, metadata=None, source_trace_id='b35c6305-b639-4670-a755-bfd5e725791f', source_observation_id=None, dataset_id='cm2ki8l2g001bu5nhzgiebjij', dataset_name='test', created_at=datetime.datetime(2024, 10, 25, 18, 30, 21, 123000, tzinfo=datetime.timezone.utc), updated_at=datetime.datetime(2024, 10, 25, 18, 30, 21, 123000, tzinfo=datetime.timezone.utc))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langfuse.create_dataset_item(\n",
    "    dataset_name=\"test\",\n",
    "    input={ \"text\": \"hello world\" },\n",
    "    expected_output={ \"text\": \"hello world\" },\n",
    "    # link to a trace\n",
    "    source_trace_id=\"b35c6305-b639-4670-a755-bfd5e725791f\",\n",
    "    # optional: link to a specific span, event, or generation\n",
    "    #source_observation_id=\"<observation_id>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meet the BigBaBy!\n"
     ]
    }
   ],
   "source": [
    "@observe()\n",
    "def meet_the_bigbaby():\n",
    "    print(\"Meet the BigBaBy!\")\n",
    "    langfuse.score(\n",
    "        trace_id=langfuse_context.get_current_trace_id(),\n",
    "        name=\"Met BB?\",\n",
    "        value=True,\n",
    "        data_type=\"BOOLEAN\",\n",
    "    )\n",
    "meet_the_bigbaby()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'五劫の擦り切れ'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langfuse.decorators import langfuse_context\n",
    "\n",
    "langfuse_context.configure(\n",
    "    public_key=public_key,\n",
    "    secret_key=secret_key,\n",
    "    host=endpoint\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "[\n",
    "    (\"system\",\"簡潔に少ない文字数で返信してください。文字数が少なければ少ないほど良いです。\"),\n",
    "    (\"user\",\"{query}\"),\n",
    "]\n",
    ")\n",
    "chat = ChatCohere(\n",
    "    model=\"command-r-plus\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "predefined_run_id = str(uuid.uuid4())\n",
    "res = chain.invoke(\n",
    "    {\"query\":\"寿限無寿限無?\"},\n",
    "    config={\n",
    "        \"callbacks\":[callback_handler],\n",
    "        \"run_id\": predefined_run_id,\n",
    "    }\n",
    ")\n",
    "\n",
    "langfuse.score(\n",
    "    trace_id=predefined_run_id,\n",
    "    name=\"content_length\",\n",
    "    value=len(res.content),\n",
    "    data_type=\"NUMERIC\",\n",
    "    comment=\"Length of AIMessage received by user\",\n",
    ")\n",
    "res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meet the BigBaBy!\n"
     ]
    }
   ],
   "source": [
    "@observe()\n",
    "def meet_the_bigbaby():\n",
    "    print(\"Meet the BigBaBy!\")\n",
    "    langfuse.score(\n",
    "        trace_id=langfuse_context.get_current_trace_id(),\n",
    "        name=\"Met BB?\",\n",
    "        value=True,\n",
    "        data_type=\"BOOLEAN\",\n",
    "    )\n",
    "meet_the_bigbaby()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse.create_dataset(name=\"japanese_holidays\")\n",
    "local_items = [\n",
    "    {\"input\": {\"year\": \"2024\",\"name\":\"海の日\"}, \"expected_output\": \"2024/07/15\"},\n",
    "    {\"input\": {\"year\": \"2024\",\"name\":\"天皇誕生日\"}, \"expected_output\": \"2024/02/23\"},\n",
    "    {\"input\": {\"year\": \"2024\",\"name\":\"勤労感謝の日\"}, \"expected_output\": \"2024/11/23\"},\n",
    "]\n",
    "for item in local_items:\n",
    "  langfuse.create_dataset_item(\n",
    "      dataset_name=\"japanese_holidays\",\n",
    "      input=item[\"input\"],\n",
    "      expected_output=item[\"expected_output\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def run_japanese_holiday_app(input, callback_handler):\n",
    "  prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"あなたは日本の祝日に詳しい有能なアシスタントです。\"),\n",
    "        (\"user\",\"{year}年の{name}をYYYY/MM/DD形式で回答してください。例えば、元日であれば「2024/01/01」と回答してください。\"),\n",
    "    ]\n",
    "  )\n",
    "  chat = ChatCohere(\n",
    "    model=\"command-r-plus\",\n",
    "    temperature=0,\n",
    "  )\n",
    "\n",
    "  chain = prompt | chat\n",
    "\n",
    "  res = chain.invoke(\n",
    "    input,\n",
    "    config={\"callbacks\":[callback_handler]}\n",
    "  )\n",
    "\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "TooManyRequestsError",
     "evalue": "status_code: 429, body: data=None message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTooManyRequestsError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mitems:\n\u001b[1;32m      4\u001b[0m   handler \u001b[38;5;241m=\u001b[39m item\u001b[38;5;241m.\u001b[39mget_langchain_handler(run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcase1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m   completion \u001b[38;5;241m=\u001b[39m \u001b[43mrun_japanese_holiday_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m   exact_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m str1, str2: str1 \u001b[38;5;241m==\u001b[39m str2\n\u001b[1;32m      9\u001b[0m   handler\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mscore(\n\u001b[1;32m     10\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexact_match\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     data_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBOOLEAN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     value\u001b[38;5;241m=\u001b[39m exact_match(completion\u001b[38;5;241m.\u001b[39mcontent, item\u001b[38;5;241m.\u001b[39mexpected_output)\n\u001b[1;32m     13\u001b[0m   )\n",
      "Cell \u001b[0;32mIn[72], line 19\u001b[0m, in \u001b[0;36mrun_japanese_holiday_app\u001b[0;34m(input, callback_handler)\u001b[0m\n\u001b[1;32m     12\u001b[0m chat \u001b[38;5;241m=\u001b[39m ChatCohere(\n\u001b[1;32m     13\u001b[0m   model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommand-r-plus\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m   temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m chat\n\u001b[0;32m---> 19\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m  \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcallback_handler\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/langchain_core/runnables/base.py:3022\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3021\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:284\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    280\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    281\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    283\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 284\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    294\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:784\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    778\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    782\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    783\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:641\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    640\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 641\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    642\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    643\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    645\u001b[0m ]\n\u001b[1;32m    646\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:631\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    630\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 631\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m         )\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    639\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:853\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 853\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    857\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/langchain_cohere/chat_models.py:597\u001b[0m, in \u001b[0;36mChatCohere._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[1;32m    594\u001b[0m request \u001b[38;5;241m=\u001b[39m get_cohere_chat_request(\n\u001b[1;32m    595\u001b[0m     messages, stop_sequences\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    596\u001b[0m )\n\u001b[0;32m--> 597\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m generation_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_generation_info(response)\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m generation_info:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/cohere/client.py:103\u001b[0m, in \u001b[0;36mexperimental_kwarg_decorator.<locals>._wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_kwarg(deprecated_kwarg, kwargs):\n\u001b[1;32m     99\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeprecated_kwarg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` parameter is an experimental feature and may change in future releases.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo suppress this warning, set `log_warning_experimental_features=False` when initializing the client.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m     )\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/cohere/client.py:35\u001b[0m, in \u001b[0;36mvalidate_args.<locals>._wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs: typing\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: typing\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m     34\u001b[0m     check_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/cohere/base_client.py:962\u001b[0m, in \u001b[0;36mBaseCohere.chat\u001b[0;34m(self, message, model, preamble, chat_history, conversation_id, prompt_truncation, connectors, search_queries_only, documents, citation_quality, temperature, max_tokens, max_input_tokens, k, p, seed, stop_sequences, frequency_penalty, presence_penalty, raw_prompting, return_prompt, tools, tool_results, force_single_step, response_format, safety_mode, request_options)\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnprocessableEntityError(\n\u001b[1;32m    959\u001b[0m         typing\u001b[38;5;241m.\u001b[39mcast(UnprocessableEntityErrorBody, construct_type(type_\u001b[38;5;241m=\u001b[39mUnprocessableEntityErrorBody, object_\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mjson()))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m:\n\u001b[0;32m--> 962\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequestsError(\n\u001b[1;32m    963\u001b[0m         typing\u001b[38;5;241m.\u001b[39mcast(TooManyRequestsErrorBody, construct_type(type_\u001b[38;5;241m=\u001b[39mTooManyRequestsErrorBody, object_\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mjson()))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    964\u001b[0m     )\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m499\u001b[39m:\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientClosedRequestError(\n\u001b[1;32m    967\u001b[0m         typing\u001b[38;5;241m.\u001b[39mcast(ClientClosedRequestErrorBody, construct_type(type_\u001b[38;5;241m=\u001b[39mClientClosedRequestErrorBody, object_\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mjson()))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    968\u001b[0m     )\n",
      "\u001b[0;31mTooManyRequestsError\u001b[0m: status_code: 429, body: data=None message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\""
     ]
    }
   ],
   "source": [
    "dataset = langfuse.get_dataset(\"japanese_holidays\")\n",
    "\n",
    "for item in dataset.items:\n",
    "  handler = item.get_langchain_handler(run_name=\"case1\")\n",
    "\n",
    "  completion = run_japanese_holiday_app(item.input, handler)\n",
    "\n",
    "  exact_match = lambda str1, str2: str1 == str2\n",
    "  handler.trace.score(\n",
    "    name=\"exact_match\",\n",
    "    data_type=\"BOOLEAN\",\n",
    "    value= exact_match(completion.content, item.expected_output)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
