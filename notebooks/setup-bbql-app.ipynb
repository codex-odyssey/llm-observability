{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# サンプルアプリのセットアップ\n",
    "\n",
    "BBQL(BigBaBy Quick Learning) のアプリケーションをセットアップします。本アプリケーションで使用している主な OSS は以下の通りです。\n",
    "\n",
    "- LangChain: 0.2.x\n",
    "- FAISS: 1.8.0\n",
    "- Langfuse: v2.75.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 必要ライブラリのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ベクトルデータベース（FAISS） のセットアップ\n",
    "\n",
    "`./docs/*.txt` に格納されているレシピデータに対して、埋め込み表現（Embeddings）を取得し、そのデータを FAISS に格納します。  \n",
    "埋め込み表現の取得には、Cohere から提供されている `embed-multilingual-v3.0` というモデルを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# ../.env を読み込みし、必要な環境変数を取得します\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "埋め込みに使うモデルを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "embeddings = CohereEmbeddings(\n",
    "    cohere_api_key=cohere_api_key,\n",
    "    model=\"embed-multilingual-v3.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ベクトルデータベース(FAISS)を宣言します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`../docs/*.txt` に格納されているテキストデータを読み込み、LangChain の Document へ変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "files = glob.glob(\"../docs/*.txt\")\n",
    "documents = []\n",
    "\n",
    "for file in files:\n",
    "    loader = TextLoader(file_path=file)\n",
    "    document = loader.load()\n",
    "    documents.extend(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "読み込んだデータを FAISS に格納します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自然言語を用いた類似度検索を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vector_store.similarity_search_with_score(query=\"カルビクッパ\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## チャットモデル(cohere.command-r-plus) + ベクトルデータベース(FAISS) を用いたシンプルな RAG 構成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自然言語で回答を生成するために、LLM のモデルを定義します。  \n",
    "ここでは、Cohere の Command R+ を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere.chat_models import ChatCohere\n",
    "\n",
    "chat = ChatCohere(\n",
    "    cohere_api_key=cohere_api_key,\n",
    "    model=\"command-r-plus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stream (LLMでトークンが出力されるたびに順次クライアントに返却する) 形式でトークンを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.stream(\"カルビクッパってどうやって作るのでしょうか？\")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "シンプルな　RAG 構成で直接 LLM を使う場合と振る舞いがどう変わるのか確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=\"\"\"\n",
    "以下のコンテキストに基づいて質問に対する回答を作成してください。\n",
    "\n",
    "## コンテキスト\n",
    "\n",
    "{context}\n",
    "\n",
    "## 質問\n",
    "\n",
    "{question}\n",
    "\"\"\")\n",
    "\n",
    "chain = (\n",
    "    {\"question\": RunnablePassthrough(), \"context\": retriever}\n",
    "    | prompt_template\n",
    "    | chat\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = chain.stream(\"カルビクッパってどうやって作るのでしょうか？\")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
