{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langfuse(Self-hosted)の機能を最低限試すためのノートブック\n",
    "\n",
    "以下の機能が対象です。\n",
    "\n",
    "- Tracing - Low-level SDK, Decorator, LangChain integration\n",
    "- Prompt Management\n",
    "- Datasets\n",
    "- Scores\n",
    "\n",
    "また、事前に以下のようにLangfuseを起動していることを前提とします。\n",
    "\n",
    "```sh\n",
    "# ~/llm-observability\n",
    "docker compose up -d\n",
    "```\n",
    "\n",
    "確認\n",
    "\n",
    "```sh\n",
    "docker compose ps\n",
    "NAME                                  IMAGE                 COMMAND                  SERVICE           CREATED      STATUS                PORTS\n",
    "llm-observability-db-1                postgres              \"docker-entrypoint.s…\"   db                7 days ago   Up 7 days (healthy)   0.0.0.0:5432->5432/tcp, :::5432->5432/tcp\n",
    "llm-observability-langfuse-server-1   langfuse/langfuse:2   \"dumb-init -- ./web/…\"   langfuse-server   7 days ago   Up 7 days             0.0.0.0:3000->3000/tcp, :::3000->3000/tcp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langfuseの初期設定\n",
    "\n",
    "適当にやってくれ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 事前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必要な情報を環境変数（`.env`）から取得します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "endpoint = os.getenv(\"ENDPOINT\")\n",
    "public_key = os.getenv(\"PUBLIC_KEY\")\n",
    "secret_key = os.getenv(\"SECRET_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langfuseのクライアントを初期化します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "langfuse = Langfuse(\n",
    "    public_key=public_key,\n",
    "    secret_key=secret_key,\n",
    "    host=endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トレース"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere.chat_models import ChatCohere\n",
    "\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "chat = ChatCohere(\n",
    "    model=\"command-r-plus\",\n",
    "    cohere_api_key=cohere_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low-level SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "trace = langfuse.trace(\n",
    "    name=\"bbql-chatbot-app\",\n",
    "    version=\"0.0.1-SNAPSHOT\",\n",
    "    tags=[\"staging\"],\n",
    "    session_id=str(uuid.uuid4())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [{\"role\": \"user\", \"content\": \"カルビクッパの作り方を教えてください\"}]\n",
    "\n",
    "generation = trace.generation(\n",
    "    name=\"generation\",\n",
    "    model=\"command-r-plus\",\n",
    "    input=input\n",
    ")\n",
    "\n",
    "chat_completion = chat.invoke(\n",
    "    input=input\n",
    ")\n",
    "\n",
    "output = chat_completion.content\n",
    "\n",
    "generation.end(\n",
    "    output=output\n",
    ")\n",
    "\n",
    "trace.update(input=input, output=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChainを使う場合は、LangChainのCallback用のHandlerがLangfuseから提供されているので、これを使うとチェーン全体のトレースが取得できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "import uuid\n",
    "\n",
    "langchain_callback = CallbackHandler(\n",
    "    host=endpoint,\n",
    "    public_key=public_key,\n",
    "    secret_key=secret_key,\n",
    "    session_id=str(uuid.uuid4())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAGの構成を作ります。詳細は、[setup-bbql-app.ipynb](./setup-bbql-app.ipynb)を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import CohereEmbeddings\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import glob\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "embeddings = CohereEmbeddings(\n",
    "    cohere_api_key=cohere_api_key,\n",
    "    model=\"embed-multilingual-v3.0\"\n",
    ")\n",
    "\n",
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "files = glob.glob(\"../app/docs/*.txt\")\n",
    "documents = []\n",
    "\n",
    "for file in files:\n",
    "    loader = TextLoader(file_path=file)\n",
    "    document = loader.load()\n",
    "    documents.extend(document)\n",
    "\n",
    "vector_store.add_documents(documents=documents)\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=\"\"\"\n",
    "以下のコンテキストに基づいて質問に対する回答を作成してください。\n",
    "\n",
    "## コンテキスト\n",
    "\n",
    "{context}\n",
    "\n",
    "## 質問\n",
    "\n",
    "{question}\n",
    "\"\"\")\n",
    "\n",
    "chain = (\n",
    "    {\"question\": RunnablePassthrough(), \"context\": retriever}\n",
    "    | prompt_template\n",
    "    | chat\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.stream(\n",
    "    \"カレーの作り方を教えてください\",\n",
    "    config={\"callbacks\": [langchain_callback]}\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain のチェーン実行に対して、自動的にトレース情報が取得されていることが確認できます。\n",
    "\n",
    "<img src=\"../images/langfuse-trace.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## プロンプト管理\n",
    "\n",
    "Langfuseでは、LLMアプリケーションが使うプロンプトをLangfuseとして構成管理することができます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "プロンプトを作成します（UIでも実施可能です）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "以下のコンテキストに基づいて質問に対する回答を作成してください。\n",
    "\n",
    "## コンテキスト\n",
    "{{context}}\n",
    "\n",
    "## 質問\n",
    "{{question}}\n",
    "\"\"\"\n",
    "\n",
    "langfuse.create_prompt(\n",
    "    name=\"bbql-app-prompt\",\n",
    "    prompt=prompt,\n",
    "    labels=[\"production\"],\n",
    "    tags=[\"production\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作成したプロンプトを読み込みます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = langfuse.get_prompt(\n",
    "    name=\"bbql-app-prompt\",\n",
    ")\n",
    "\n",
    "print(prompt.prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変数のバインド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_prompt = prompt.compile(\n",
    "    context=\"雰囲気で作れ！\",\n",
    "    question=\"カレーの作り方を教えてください\"\n",
    ")\n",
    "\n",
    "print(compiled_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChainから使う場合は、`get_langchain_prompt`が使えるのでこちらを使うと良い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_prompt = prompt.get_langchain_prompt()\n",
    "\n",
    "print(langchain_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAGの中で使うならこんな感じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"question\": RunnablePassthrough(), \"context\": retriever}\n",
    "    | PromptTemplate.from_template(langchain_prompt)\n",
    "    | chat\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = chain.stream(\n",
    "    \"カレーの作り方を教えてください\",\n",
    "    config={\"callbacks\": [langchain_callback]}\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "プロンプトは、バージョン管理可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = \"\"\"\n",
    "以下のコンテキストに基づいて質問に対する回答をBBっぽく作成してください。\n",
    "\n",
    "## コンテキスト\n",
    "{{context}}\n",
    "\n",
    "## 質問\n",
    "{{question}}\n",
    "\"\"\"\n",
    "\n",
    "# 同一名でプロンプトを作成すると、上書きされ Version が上がる\n",
    "langfuse.create_prompt(\n",
    "    name=\"bbql-app-prompt\",\n",
    "    prompt=prompt2,\n",
    "    labels=[\"production\"],\n",
    "    tags=[\"production\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version: 1, Version: 2のプロンプトを参照してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_v1 = langfuse.get_prompt(\n",
    "    name=\"bbql-app-prompt\",\n",
    "    version=1\n",
    ")\n",
    "\n",
    "prompt_v2 = langfuse.get_prompt(\n",
    "    name=\"bbql-app-prompt\",\n",
    "    version=2\n",
    ")\n",
    "\n",
    "print(f\"prompt_v1: {prompt_v1.prompt}\")\n",
    "print(\"*********************************\")\n",
    "print(f\"prompt_v2: {prompt_v2.prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "プロンプトをLLMアプリケーションのコード中に埋め込むと、プロンプトの変更 = LLMアプリケーションの再ビルドになるので結構手間となる。  \n",
    "かつ、LLMアプリケーションはプロンプトだけ細かくチューニングしたいパターンが結構あるので、こういう機能が備わってくれるとありがたい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"キノコを使ったおつまみとかないですかね\"\n",
    "\n",
    "response = chain.stream(\n",
    "    query,\n",
    "    config={\"callbacks\": [langchain_callback]}\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk, end=\"\")\n",
    "\n",
    "current_trace_id = langchain_callback.get_trace_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse.score(\n",
    "    name=\"bbql-app-sample-feedback\",\n",
    "    value=1,\n",
    "    data_type=\"NUMERIC\",\n",
    "    trace_id=current_trace_id,\n",
    "    comment=\"求めていたものでした\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UIで目的のトレースを確認すると、評価がつけられていることが確認できる\n",
    "\n",
    "<img src=\"../images/langfuse-score.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の例は、UI等で収集した人による評価の想定だが、LLM自身に評価をさせることもできる（いわゆる、LLM-as-a-Judge）  \n",
    "ここでは、LangChainのEvaluatorを使用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.loading import load_evaluator\n",
    "from langchain.evaluation.schema import EvaluatorType\n",
    "\n",
    "# Embedding(ベクトル空間に埋め込まれた文書間の類似性)に基づく評価を行う\n",
    "evaluator = load_evaluator(\n",
    "    evaluator=EvaluatorType.EMBEDDING_DISTANCE,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "# 評価対象のトレースを取得する\n",
    "trace = langfuse.get_trace(id=current_trace_id)\n",
    "\n",
    "# LangChainのEvaluatorを用いて評価をする\n",
    "result = evaluator.evaluate_strings(\n",
    "    prediction=trace.output,\n",
    "    reference=query\n",
    ")\n",
    "\n",
    "# 評価に基づき、スコアとトレースを紐づける\n",
    "langfuse.score(\n",
    "    name=\"embedding-distance\",\n",
    "    value=result[\"score\"],\n",
    "    data_type=\"NUMERIC\",\n",
    "    trace_id=current_trace_id,\n",
    "    comment=\"by embedding distance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EvaluatorTypeを参照すると、使用可能なEvaluatorが数多く提供されていることが確認できる\n",
    "\n",
    "```py\n",
    "    QA = \"qa\"\n",
    "    \"\"\"Question answering evaluator, which grades answers to questions\n",
    "    directly using an LLM.\"\"\"\n",
    "    COT_QA = \"cot_qa\"\n",
    "    \"\"\"Chain of thought question answering evaluator, which grades\n",
    "    answers to questions using\n",
    "    chain of thought 'reasoning'.\"\"\"\n",
    "    CONTEXT_QA = \"context_qa\"\n",
    "    \"\"\"Question answering evaluator that incorporates 'context' in the response.\"\"\"\n",
    "    PAIRWISE_STRING = \"pairwise_string\"\n",
    "    \"\"\"The pairwise string evaluator, which predicts the preferred prediction from\n",
    "    between two models.\"\"\"\n",
    "    SCORE_STRING = \"score_string\"\n",
    "    \"\"\"The scored string evaluator, which gives a score between 1 and 10 \n",
    "    to a prediction.\"\"\"\n",
    "    LABELED_PAIRWISE_STRING = \"labeled_pairwise_string\"\n",
    "    \"\"\"The labeled pairwise string evaluator, which predicts the preferred prediction\n",
    "    from between two models based on a ground truth reference label.\"\"\"\n",
    "    LABELED_SCORE_STRING = \"labeled_score_string\"\n",
    "    \"\"\"The labeled scored string evaluator, which gives a score between 1 and 10\n",
    "    to a prediction based on a ground truth reference label.\"\"\"\n",
    "    AGENT_TRAJECTORY = \"trajectory\"\n",
    "    \"\"\"The agent trajectory evaluator, which grades the agent's intermediate steps.\"\"\"\n",
    "    CRITERIA = \"criteria\"\n",
    "    \"\"\"The criteria evaluator, which evaluates a model based on a\n",
    "    custom set of criteria without any reference labels.\"\"\"\n",
    "    LABELED_CRITERIA = \"labeled_criteria\"\n",
    "    \"\"\"The labeled criteria evaluator, which evaluates a model based on a\n",
    "    custom set of criteria, with a reference label.\"\"\"\n",
    "    STRING_DISTANCE = \"string_distance\"\n",
    "    \"\"\"Compare predictions to a reference answer using string edit distances.\"\"\"\n",
    "    EXACT_MATCH = \"exact_match\"\n",
    "    \"\"\"Compare predictions to a reference answer using exact matching.\"\"\"\n",
    "    REGEX_MATCH = \"regex_match\"\n",
    "    \"\"\"Compare predictions to a reference answer using regular expressions.\"\"\"\n",
    "    PAIRWISE_STRING_DISTANCE = \"pairwise_string_distance\"\n",
    "    \"\"\"Compare predictions based on string edit distances.\"\"\"\n",
    "    EMBEDDING_DISTANCE = \"embedding_distance\"\n",
    "    \"\"\"Compare a prediction to a reference label using embedding distance.\"\"\"\n",
    "    PAIRWISE_EMBEDDING_DISTANCE = \"pairwise_embedding_distance\"\n",
    "    \"\"\"Compare two predictions using embedding distance.\"\"\"\n",
    "    JSON_VALIDITY = \"json_validity\"\n",
    "    \"\"\"Check if a prediction is valid JSON.\"\"\"\n",
    "    JSON_EQUALITY = \"json_equality\"\n",
    "    \"\"\"Check if a prediction is equal to a reference JSON.\"\"\"\n",
    "    JSON_EDIT_DISTANCE = \"json_edit_distance\"\n",
    "    \"\"\"Compute the edit distance between two JSON strings after canonicalization.\"\"\"\n",
    "    JSON_SCHEMA_VALIDATION = \"json_schema_validation\"\n",
    "    \"\"\"Check if a prediction is valid JSON according to a JSON schema.\"\"\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
